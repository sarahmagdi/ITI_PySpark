{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05f1e8db",
   "metadata": {},
   "source": [
    "# **Labs 1 and 2 PySpark:**\n",
    "\n",
    "In these labs we will be using the \"[[NeurIPS 2020] Data Science for COVID-19 (DS4C)](https://www.kaggle.com/datasets/kimjihoo/coronavirusdataset?select=PatientInfo.csv)\" dataset, retrieved from [Kaggle](https://www.kaggle.com/) on 1/6/2022, for educational non commercial purpose, License\n",
    "[CC BY-NC-SA 4.0\n",
    "](https://creativecommons.org/licenses/by-nc-sa/4.0/)\n",
    "\n",
    "\n",
    "The csv file that we will be using in this lab is **PatientInfo**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac9bbf0",
   "metadata": {},
   "source": [
    "## PatientInfo.csv\n",
    "\n",
    "**patient_id**\n",
    "the ID of the patient\n",
    "\n",
    "**sex**\n",
    "the sex of the patient\n",
    "\n",
    "**age**\n",
    "the age of the patient\n",
    "\n",
    "**country**\n",
    "the country of the patient\n",
    "\n",
    "**province**\n",
    "the province of the patient\n",
    "\n",
    "**city**\n",
    "the city of the patient\n",
    "\n",
    "**infection_case**\n",
    "the case of infection\n",
    "\n",
    "**infected_by**\n",
    "the ID of who infected the patient\n",
    "\n",
    "\n",
    "**contact_number**\n",
    "the number of contacts with people\n",
    "\n",
    "**symptom_onset_date**\n",
    "the date of symptom onset\n",
    "\n",
    "**confirmed_date**\n",
    "the date of being confirmed\n",
    "\n",
    "**released_date**\n",
    "the date of being released\n",
    "\n",
    "**deceased_date**\n",
    "the date of being deceased\n",
    "\n",
    "**state**\n",
    "isolated / released / deceased"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b6f619",
   "metadata": {},
   "source": [
    "### Import the pyspark and check it's version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29fe2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f24894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1.2\n"
     ]
    }
   ],
   "source": [
    "print(spark.version)\n",
    "#print(SparkContext.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c0fc8e",
   "metadata": {},
   "source": [
    "### Import and create SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94904228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('lab1-2').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356d68d",
   "metadata": {},
   "source": [
    "### Load the PatientInfo.csv file and show the first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5185728",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2196bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('PatientInfo.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebb91401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------+--------+-----------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+\n",
      "|patient_id|sex   |age|country|province|city       |infection_case      |infected_by|contact_number|symptom_onset_date|confirmed_date|released_date|deceased_date|state   |\n",
      "+----------+------+---+-------+--------+-----------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+\n",
      "|1000000001|male  |50s|Korea  |Seoul   |Gangseo-gu |overseas inflow     |null       |75            |2020-01-22        |2020-01-23    |2020-02-05   |null         |released|\n",
      "|1000000002|male  |30s|Korea  |Seoul   |Jungnang-gu|overseas inflow     |null       |31            |null              |2020-01-30    |2020-03-02   |null         |released|\n",
      "|1000000003|male  |50s|Korea  |Seoul   |Jongno-gu  |contact with patient|2002000001 |17            |null              |2020-01-30    |2020-02-19   |null         |released|\n",
      "|1000000004|male  |20s|Korea  |Seoul   |Mapo-gu    |overseas inflow     |null       |9             |2020-01-26        |2020-01-30    |2020-02-15   |null         |released|\n",
      "|1000000005|female|20s|Korea  |Seoul   |Seongbuk-gu|contact with patient|1000000002 |2             |null              |2020-01-31    |2020-02-24   |null         |released|\n",
      "+----------+------+---+-------+--------+-----------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d43bdca",
   "metadata": {},
   "source": [
    "### Display the schema of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c51a2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- infected_by: string (nullable = true)\n",
      " |-- contact_number: string (nullable = true)\n",
      " |-- symptom_onset_date: string (nullable = true)\n",
      " |-- confirmed_date: string (nullable = true)\n",
      " |-- released_date: string (nullable = true)\n",
      " |-- deceased_date: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3114e1dd",
   "metadata": {},
   "source": [
    "### Display the statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d63947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+----+----------+--------+--------------+--------------------------+--------------------+--------------------+------------------+--------------+-------------+-------------+--------+\n",
      "|summary|patient_id          |sex   |age |country   |province|city          |infection_case            |infected_by         |contact_number      |symptom_onset_date|confirmed_date|released_date|deceased_date|state   |\n",
      "+-------+--------------------+------+----+----------+--------+--------------+--------------------------+--------------------+--------------------+------------------+--------------+-------------+-------------+--------+\n",
      "|count  |5165                |4043  |3785|5165      |5165    |5071          |4246                      |1346                |791                 |690               |5162          |1587         |66           |5165    |\n",
      "|mean   |2.8636345618679576E9|null  |null|null      |null    |null          |null                      |2.2845944015643125E9|1.6772572523506988E7|null              |null          |null         |null         |null    |\n",
      "|stddev |2.074210725277473E9 |null  |null|null      |null    |null          |null                      |1.5265072953383324E9|3.093097580985502E8 |null              |null          |null         |null         |null    |\n",
      "|min    |1000000001          |female|0s  |Bangladesh|Busan   |Andong-si     |Anyang Gunpo Pastors Group|1000000002          |-                   |                  |2020-01-20    |2020-02-05   |2020-02-19   |deceased|\n",
      "|max    |7000000019          |male  |90s |Vietnam   |Ulsan   |sankyeock-dong|overseas inflow           |7000000009          |95                  |2020-06-28        |2020-06-30    |2020-06-28   |2020-05-25   |released|\n",
      "+-------+--------------------+------+----+----------+--------+--------------+--------------------------+--------------------+--------------------+------------------+--------------+-------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0896a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------+----+----------+--------+--------------+--------------------+--------------------+\n",
      "|summary|          patient_id|   sex| age|   country|province|          city|      infection_case|         infected_by|\n",
      "+-------+--------------------+------+----+----------+--------+--------------+--------------------+--------------------+\n",
      "|  count|                5165|  4043|3785|      5165|    5165|          5071|                4246|                1346|\n",
      "|   mean|2.8636345618679576E9|  null|null|      null|    null|          null|                null|2.2845944015643125E9|\n",
      "| stddev| 2.074210725277473E9|  null|null|      null|    null|          null|                null|1.5265072953383324E9|\n",
      "|    min|          1000000001|female|  0s|Bangladesh|   Busan|     Andong-si|Anyang Gunpo Past...|          1000000002|\n",
      "|    max|          7000000019|  male| 90s|   Vietnam|   Ulsan|sankyeock-dong|     overseas inflow|          7000000009|\n",
      "+-------+--------------------+------+----+----------+--------+--------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe([\"patient_id\",\"sex\"   ,\"age\" ,\"country\" ,  \"province\",\"city\",\"infection_case\" ,\"infected_by\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91ba1f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------------+--------------+-------------+-------------+--------+\n",
      "|summary|      contact_number|symptom_onset_date|confirmed_date|released_date|deceased_date|   state|\n",
      "+-------+--------------------+------------------+--------------+-------------+-------------+--------+\n",
      "|  count|                 791|               690|          5162|         1587|           66|    5165|\n",
      "|   mean|1.6772572523506988E7|              null|          null|         null|         null|    null|\n",
      "| stddev| 3.093097580985502E8|              null|          null|         null|         null|    null|\n",
      "|    min|                   -|                  |    2020-01-20|   2020-02-05|   2020-02-19|deceased|\n",
      "|    max|                  95|        2020-06-28|    2020-06-30|   2020-06-28|   2020-05-25|released|\n",
      "+-------+--------------------+------------------+--------------+-------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe([\"contact_number\" , \"symptom_onset_date\",\"confirmed_date\",\"released_date\",\"deceased_date\",\"state\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78859b9",
   "metadata": {},
   "source": [
    "### Using the state column.\n",
    "### How many people survived (released), and how many didn't survive (isolated/deceased)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77729f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63e84045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|   state|people count|\n",
      "+--------+------------+\n",
      "|isolated|        2158|\n",
      "|released|        2929|\n",
      "|deceased|          78|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('state').agg(count(\"patient_id\").alias(\"people count\")).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fce9e8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "survived people count : 2929\n",
      "unSurvived people count : 2236\n"
     ]
    }
   ],
   "source": [
    "print(\"survived people count :\",(df.select(\"state\").where(col(\"state\") == \"released\")).count())\n",
    "print(\"unSurvived people count :\",df.filter((col(\"state\") == \"isolated\") | (col(\"state\") == \"deceased\")).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c16346",
   "metadata": {},
   "source": [
    "### Display the number of null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4adc0ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+----+-------+--------+----+--------------+-----------+--------------+------------------+--------------+-------------+-------------+-----+\n",
      "|patient_id| sex| age|country|province|city|infection_case|infected_by|contact_number|symptom_onset_date|confirmed_date|released_date|deceased_date|state|\n",
      "+----------+----+----+-------+--------+----+--------------+-----------+--------------+------------------+--------------+-------------+-------------+-----+\n",
      "|         0|1122|1380|      0|       0|  94|           919|       3819|          4374|              4475|             3|         3578|         5099|    0|\n",
      "+----------+----+----+-------+--------+----+--------------+-----------+--------------+------------------+--------------+-------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d375d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5165"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84d24e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nulls in each columns\n",
      "+----------+----+----+-------+--------+----+--------------+-----------+\n",
      "|patient_id| sex| age|country|province|city|infection_case|infected_by|\n",
      "+----------+----+----+-------+--------+----+--------------+-----------+\n",
      "|         0|1122|1380|      0|       0|  94|           919|       3819|\n",
      "+----------+----+----+-------+--------+----+--------------+-----------+\n",
      "\n",
      "+--------------+------------------+--------------+-------------+-------------+-----+\n",
      "|contact_number|symptom_onset_date|confirmed_date|released_date|deceased_date|state|\n",
      "+--------------+------------------+--------------+-------------+-------------+-----+\n",
      "|          4374|              4475|             3|         3578|         5099|    0|\n",
      "+--------------+------------------+--------------+-------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"nulls in each columns\")\n",
    "l=[\"patient_id\",\"sex\"   ,\"age\" ,\"country\" ,  \"province\",\"city\",\"infection_case\" ,\"infected_by\"]\n",
    "l2=[\"contact_number\" , \"symptom_onset_date\",\"confirmed_date\",\"released_date\",\"deceased_date\",\"state\"]\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in l]).show()\n",
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in l2]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3133f5a",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec32fa0c",
   "metadata": {},
   "source": [
    "### Fill the nulls in the deceased_date with the released_date. \n",
    "- You can use <b>coalesce</b> function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7899f138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5ba64f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The Coalesce function reduces the number of partitions in the PySpark Data Frame\n",
    "df2 = df.coalesce(5165)\n",
    "#df2.count()\n",
    "df2.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beb02d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to create anew column deceased_date (nulls in the deceased_date fill with  values in the released_date) to ,from\n",
    "from pyspark.sql.functions import coalesce\n",
    "df=df.withColumn(\"deceased_date\",coalesce(df.deceased_date,df.released_date)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5075d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------+-------------+-------------+-----+\n",
      "|contact_number|symptom_onset_date|confirmed_date|released_date|deceased_date|state|\n",
      "+--------------+------------------+--------------+-------------+-------------+-----+\n",
      "|          4374|              4475|             3|         3578|         3514|    0|\n",
      "+--------------+------------------+--------------+-------------+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in l2]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "70bde9e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nbefore\\nA|B\\n0,1\\n2,null\\n3,null\\n4,2\\nafter\\nA|B\\n0,1\\n2,2\\n3,3\\n4,2\\n\\nfrom a to b\\nfrom pyspark.sql.functions import coalesce   \\ndf.withColumn(\"B\",coalesce(df.B,df.A)) #to,from\\n\\nanother way \\nfrom to \\ndf.na.fill(df.A,\"B\")\\n'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "before\n",
    "A|B\n",
    "0,1\n",
    "2,null\n",
    "3,null\n",
    "4,2\n",
    "after\n",
    "A|B\n",
    "0,1\n",
    "2,2\n",
    "3,3\n",
    "4,2\n",
    "\n",
    "from a to b\n",
    "from pyspark.sql.functions import coalesce   \n",
    "df.withColumn(\"B\",coalesce(df.B,df.A)) #to,from\n",
    "\n",
    "another way \n",
    "from to \n",
    "df.na.fill(df.A,\"B\")\n",
    "3-\n",
    "df1.select('A',\n",
    "           when( df1.B.isNull(), df1.A).otherwise(df1.B).alias('B')\n",
    "          )\\\n",
    "   .show()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec324f5",
   "metadata": {},
   "source": [
    "### Add a column named no_days which is difference between the deceased_date and the confirmed_date then show the top 5 rows. Print the schema.\n",
    "- <b> Hint: You need to typecast these columns as date first <b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c42e8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "df=df.withColumn(\"deceased_date\",col(\"deceased_date\").cast(DateType())) \\\n",
    ".withColumn(\"confirmed_date\",col(\"confirmed_date\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "265ddc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- infected_by: string (nullable = true)\n",
      " |-- contact_number: string (nullable = true)\n",
      " |-- symptom_onset_date: string (nullable = true)\n",
      " |-- confirmed_date: date (nullable = true)\n",
      " |-- released_date: string (nullable = true)\n",
      " |-- deceased_date: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65336113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df2=df.withColumn(\"no_days\",datediff(col(\"deceased_date\"),col(\"confirmed_date\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9bb678e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+--------------+\n",
      "|no_days|deceased_date|confirmed_date|\n",
      "+-------+-------------+--------------+\n",
      "|     13|   2020-02-05|    2020-01-23|\n",
      "|     32|   2020-03-02|    2020-01-30|\n",
      "|     20|   2020-02-19|    2020-01-30|\n",
      "|     16|   2020-02-15|    2020-01-30|\n",
      "|     24|   2020-02-24|    2020-01-31|\n",
      "+-------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select([\"no_days\",\"deceased_date\",\"confirmed_date\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47f2a853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------+--------+-----------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+---------------+\n",
      "|patient_id|sex   |age|country|province|city       |infection_case      |infected_by|contact_number|symptom_onset_date|confirmed_date|released_date|deceased_date|state   |no_days        |\n",
      "+----------+------+---+-------+--------+-----------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+---------------+\n",
      "|1000000001|male  |50s|Korea  |Seoul   |Gangseo-gu |overseas inflow     |null       |75            |2020-01-22        |2020-01-23    |2020-02-05   |2020-02-05   |released|13 days        |\n",
      "|1000000002|male  |30s|Korea  |Seoul   |Jungnang-gu|overseas inflow     |null       |31            |null              |2020-01-30    |2020-03-02   |2020-03-02   |released|1 months 2 days|\n",
      "|1000000003|male  |50s|Korea  |Seoul   |Jongno-gu  |contact with patient|2002000001 |17            |null              |2020-01-30    |2020-02-19   |2020-02-19   |released|20 days        |\n",
      "|1000000004|male  |20s|Korea  |Seoul   |Mapo-gu    |overseas inflow     |null       |9             |2020-01-26        |2020-01-30    |2020-02-15   |2020-02-15   |released|16 days        |\n",
      "|1000000005|female|20s|Korea  |Seoul   |Seongbuk-gu|contact with patient|1000000002 |2             |null              |2020-01-31    |2020-02-24   |2020-02-24   |released|24 days        |\n",
      "+----------+------+---+-------+--------+-----------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d065e472",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nadjustedDf = df.withColumn(\"deceased_date\", to_date(col(\"deceased_date\"), \"yyyy-MM-d\")).withColumn(\"confirmed_date\", to_date(col(\"confirmed_date\"), \"yyyy-MM-d\"))\\n    .withColumn(\\'no_days\\', col(\"deceased_date\")-col(\"confirmed_date\"))\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "adjustedDf = df.withColumn(\"deceased_date\", to_date(col(\"deceased_date\"), \"yyyy-MM-d\"))\\\n",
    ".withColumn(\"confirmed_date\", to_date(col(\"confirmed_date\"), \"yyyy-MM-d\"))\n",
    "    .withColumn('no_days', col(\"deceased_date\")-col(\"confirmed_date\"))\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967b2466",
   "metadata": {},
   "source": [
    "### Add a is_male column if male then it should yield true, else then False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8efe03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.withColumn(\"is_male\",when( (col(\"sex\") == \"male\"), True).otherwise(False)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ccf3ee9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+---+-------+--------+-----------+---------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+-------+-------+\n",
      "|patient_id|sex |age|country|province|city       |infection_case |infected_by|contact_number|symptom_onset_date|confirmed_date|released_date|deceased_date|state   |no_days|is_male|\n",
      "+----------+----+---+-------+--------+-----------+---------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+-------+-------+\n",
      "|1000000001|male|50s|Korea  |Seoul   |Gangseo-gu |overseas inflow|null       |75            |2020-01-22        |2020-01-23    |2020-02-05   |2020-02-05   |released|13     |true   |\n",
      "|1000000002|male|30s|Korea  |Seoul   |Jungnang-gu|overseas inflow|null       |31            |null              |2020-01-30    |2020-03-02   |2020-03-02   |released|32     |true   |\n",
      "+----------+----+---+-------+--------+-----------+---------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+-------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(2,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "64d03f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- infected_by: string (nullable = true)\n",
      " |-- contact_number: string (nullable = true)\n",
      " |-- symptom_onset_date: string (nullable = true)\n",
      " |-- confirmed_date: date (nullable = true)\n",
      " |-- released_date: string (nullable = true)\n",
      " |-- deceased_date: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- no_days: integer (nullable = true)\n",
      " |-- is_male: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8603d7c0",
   "metadata": {},
   "source": [
    "### Add a is_dead column if patient state is not released then it should yield true, else then False\n",
    "\n",
    "- Use <b>UDF</b> to perform this task. \n",
    "- However, UDF is not recommended there is no built in function can do the required operation.\n",
    "- UDF is slower than built in functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "718eb044",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import BooleanType,StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df74b5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3343075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df2=df2.withColumn(\"is_dead\",when( (col(\"state\") == \"released\"), False).otherwise(True)) \n",
    "#User Defined Function (UDF)\n",
    "def is_dead_fun(x):\n",
    "    if x == \"released\":\n",
    "        y=False\n",
    "    else:\n",
    "        y=True\n",
    "    return y\n",
    "deadUDF = udf(lambda z:is_dead_fun(z),BooleanType())   \n",
    "\n",
    "df2=df2.withColumn(\"is_deade\", deadUDF(col(\"state\"))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d6473d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- infected_by: string (nullable = true)\n",
      " |-- contact_number: string (nullable = true)\n",
      " |-- symptom_onset_date: string (nullable = true)\n",
      " |-- confirmed_date: date (nullable = true)\n",
      " |-- released_date: string (nullable = true)\n",
      " |-- deceased_date: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- no_days: interval (nullable = true)\n",
      " |-- is_male: boolean (nullable = false)\n",
      " |-- is_deade: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bed0aa55",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o396.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 224) (basamoka.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3960/1627261745.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \"\"\"\n\u001b[0;32m    483\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 484\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    485\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o396.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 224) (basamoka.mshome.net executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:472)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:425)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:47)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.evaluate(BatchEvalPythonExec.scala:81)\r\n\tat org.apache.spark.sql.execution.python.EvalPythonExec.$anonfun$doExecute$2(EvalPythonExec.scala:130)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2(RDD.scala:863)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitions$2$adapted(RDD.scala:863)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 24 more\r\n"
     ]
    }
   ],
   "source": [
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faa39a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.drop(\"is_deade\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d17d2ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.withColumn(\"is_dead\",when( (col(\"state\") == \"released\"), False).otherwise(True)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d25ea5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|is_dead|   state|\n",
      "+-------+--------+\n",
      "|  false|released|\n",
      "|  false|released|\n",
      "|  false|released|\n",
      "|  false|released|\n",
      "|  false|released|\n",
      "+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select([\"is_dead\",\"state\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "25d37421",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/D:/iti studing core/spark/lab1-2/PatientInfo_modified.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3960/3114693333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#df2.repartition(1).write.csv(\"PatientInfo_modified.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#df2.coalesce(1).write.csv(\"PatientInfo_modified.csv\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'com.databricks.spark.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'PatientInfo_modified.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1107\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1108\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1109\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1111\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/D:/iti studing core/spark/lab1-2/PatientInfo_modified.csv already exists."
     ]
    }
   ],
   "source": [
    "#df2.write.csv(\"PatientInfo_modified.csv\")\n",
    "\n",
    "#df2.toPandas().to_csv(\"PatientInfo_modified.csv\")\n",
    "#df2.repartition(1).write.csv(\"PatientInfo_modified.csv\")\n",
    "#df2.coalesce(1).write.csv(\"PatientInfo_modified.csv\")\n",
    "df2.write.format('com.databricks.spark.csv').save('PatientInfo_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "42ef5b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf2.write.mode(\"overwrite\").csv(r\"D:\\\\iti studing core\\\\spark\\\\lab1-2\\\\PatientInfo_modified\")\\n/////\\nfrom pyspark.sql.functions import udf\\nfrom pyspark.sql.types import StringType\\n\\ndef array_to_string(my_list):\\n    return \\'[\\' + \\',\\'.join([str(elem) for elem in my_list]) + \\']\\'\\n\\narray_to_string_udf = udf(array_to_string,StringType())\\n\\ndf3 = df2.withColumn(\\'state-stringified\\',array_to_string_udf(df2[\"state\"]))\\nll=[\"is_dead\",\"is_male\"]\\nll.extend(l)\\nll.extend(l2)\\ndf3.drop(ll).write.csv(\"dd.csv\")\\n////\\ndf2.select(\\n    \"age\", \"city\", \"confirmed_date\", \"contact_number\", \"country\", \"deceased_date\", \"infected_by\", \"infection_case\", \"is_dead\", \"is_male\", \"no_days\", \"patient_id\", \"province\", \"released_date\", \"sex\", \"state\", \"symptom_onset_date\"\\n    ).write.csv(\"good_loc.csv\")\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#same error: Cannot save interval data type into external storage.\n",
    "\"\"\"\n",
    "df2.write.mode(\"overwrite\").csv(r\"D:\\iti studing core\\spark\\lab1-2\\PatientInfo_modified\")\n",
    "/////\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def array_to_string(my_list):\n",
    "    return '[' + ','.join([str(elem) for elem in my_list]) + ']'\n",
    "\n",
    "array_to_string_udf = udf(array_to_string,StringType())\n",
    "\n",
    "df3 = df2.withColumn('state-stringified',array_to_string_udf(df2[\"state\"]))\n",
    "ll=[\"is_dead\",\"is_male\"]\n",
    "ll.extend(l)\n",
    "ll.extend(l2)\n",
    "df3.drop(ll).write.csv(\"dd.csv\")\n",
    "////\n",
    "df2.select(\n",
    "    \"age\", \"city\", \"confirmed_date\", \"contact_number\", \"country\", \"deceased_date\", \"infected_by\", \"infection_case\", \"is_dead\", \"is_male\", \"no_days\", \"patient_id\", \"province\", \"released_date\", \"sex\", \"state\", \"symptom_onset_date\"\n",
    "    ).write.csv(\"good_loc.csv\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15264a9",
   "metadata": {},
   "source": [
    "### Change the ages to bins from 10s, 0s, 10s, 20s,.etc to 0,10, 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e0a8440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|50s|\n",
      "|30s|\n",
      "|50s|\n",
      "|20s|\n",
      "|20s|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\"age\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a2b57ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "|50 |\n",
      "|30 |\n",
      "|50 |\n",
      "|20 |\n",
      "|20 |\n",
      "|50 |\n",
      "|20 |\n",
      "|20 |\n",
      "|30 |\n",
      "|60 |\n",
      "+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "df2=df2.withColumn('age', translate('age', 's', ''))\n",
    "\n",
    "df2.select('age').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43cdf846",
   "metadata": {},
   "source": [
    "### Change age, and no_days  to be typecasted as Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfea7fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.types as t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e86b8947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.withColumn(\"age\",col(\"age\").cast(t.DoubleType())) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47ef60a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.withColumn(\"no_days\",col(\"no_days\").cast(t.DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7c9f63e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- patient_id: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- infection_case: string (nullable = true)\n",
      " |-- infected_by: string (nullable = true)\n",
      " |-- contact_number: string (nullable = true)\n",
      " |-- symptom_onset_date: string (nullable = true)\n",
      " |-- confirmed_date: date (nullable = true)\n",
      " |-- released_date: string (nullable = true)\n",
      " |-- deceased_date: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- no_days: double (nullable = true)\n",
      " |-- is_male: boolean (nullable = false)\n",
      " |-- is_dead: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e9a28c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|no_days|\n",
      "+-------+\n",
      "|13.0   |\n",
      "|32.0   |\n",
      "|20.0   |\n",
      "|16.0   |\n",
      "|24.0   |\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('no_days').show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6a6626",
   "metadata": {},
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e3a87",
   "metadata": {},
   "source": [
    "### Drop the columns\n",
    "[\"patient_id\",\"sex\",\"infected_by\",\"contact_number\",\"released_date\",\"state\",\n",
    "\"symptom_onset_date\",\"confirmed_date\",\"deceased_date\",\"country\",\"no_days\",\n",
    "\"city\",\"infection_case\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3ae0c428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=df2.drop(\"patient_id\",\"sex\",\"infected_by\",\"contact_number\",\"released_date\",\"state\", \"symptom_onset_date\",\"confirmed_date\",\"deceased_date\",\"country\",\"no_days\", \"city\",\"infection_case\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "48c09262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: double (nullable = true)\n",
      " |-- province: string (nullable = true)\n",
      " |-- is_male: boolean (nullable = false)\n",
      " |-- is_dead: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32df47a6",
   "metadata": {},
   "source": [
    "### Recount the number of nulls now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6d66eaa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "| age|province|\n",
      "+----+--------+\n",
      "|1380|       0|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#df2.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df2.columns] ).show()\n",
    "l4=[\"age\",\"province\"]\n",
    "df2.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in l4] ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2423b980",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndf2.select([count(when(col(c).contains('None') |                             col(c).contains('NULL') |                             (col(c) == '' ) |                             col(c).isNull() |                             isnan(c), c \\n                           )).alias(c)\\n                    for c in l4]).show()\\n\\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df2.select([count(when(col(c).contains('None') | \\\n",
    "                            col(c).contains('NULL') | \\\n",
    "                            (col(c) == '' ) | \\\n",
    "                            col(c).isNull() | \\\n",
    "                            isnan(c), c \n",
    "                           )).alias(c)\n",
    "                    for c in l4]).show()\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc71f158",
   "metadata": {},
   "source": [
    "## Now do the same but using SQL select statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d540282",
   "metadata": {},
   "source": [
    "### From the original Patient DataFrame, Create a temporary view (table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf56099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"patients_tabel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb004277",
   "metadata": {},
   "source": [
    "### Use SELECT statement to select all columns from the dataframe and show the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12ae8226",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------+--------+------------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+\n",
      "|patient_id|   sex|age|country|province|        city|      infection_case|infected_by|contact_number|symptom_onset_date|confirmed_date|released_date|deceased_date|   state|\n",
      "+----------+------+---+-------+--------+------------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+\n",
      "|1000000001|  male|50s|  Korea|   Seoul|  Gangseo-gu|     overseas inflow|       null|            75|        2020-01-22|    2020-01-23|   2020-02-05|         null|released|\n",
      "|1000000002|  male|30s|  Korea|   Seoul| Jungnang-gu|     overseas inflow|       null|            31|              null|    2020-01-30|   2020-03-02|         null|released|\n",
      "|1000000003|  male|50s|  Korea|   Seoul|   Jongno-gu|contact with patient| 2002000001|            17|              null|    2020-01-30|   2020-02-19|         null|released|\n",
      "|1000000004|  male|20s|  Korea|   Seoul|     Mapo-gu|     overseas inflow|       null|             9|        2020-01-26|    2020-01-30|   2020-02-15|         null|released|\n",
      "|1000000005|female|20s|  Korea|   Seoul| Seongbuk-gu|contact with patient| 1000000002|             2|              null|    2020-01-31|   2020-02-24|         null|released|\n",
      "|1000000006|female|50s|  Korea|   Seoul|   Jongno-gu|contact with patient| 1000000003|            43|              null|    2020-01-31|   2020-02-19|         null|released|\n",
      "|1000000007|  male|20s|  Korea|   Seoul|   Jongno-gu|contact with patient| 1000000003|             0|              null|    2020-01-31|   2020-02-10|         null|released|\n",
      "|1000000008|  male|20s|  Korea|   Seoul|         etc|     overseas inflow|       null|             0|              null|    2020-02-02|   2020-02-24|         null|released|\n",
      "|1000000009|  male|30s|  Korea|   Seoul|   Songpa-gu|     overseas inflow|       null|            68|              null|    2020-02-05|   2020-02-21|         null|released|\n",
      "|1000000010|female|60s|  Korea|   Seoul| Seongbuk-gu|contact with patient| 1000000003|             6|              null|    2020-02-05|   2020-02-29|         null|released|\n",
      "|1000000011|female|50s|  China|   Seoul|Seodaemun-gu|     overseas inflow|       null|            23|              null|    2020-02-06|   2020-02-29|         null|released|\n",
      "|1000000012|  male|20s|  Korea|   Seoul|         etc|     overseas inflow|       null|             0|              null|    2020-02-07|   2020-02-27|         null|released|\n",
      "|1000000013|  male|80s|  Korea|   Seoul|   Jongno-gu|contact with patient| 1000000017|           117|              null|    2020-02-16|         null|         null|deceased|\n",
      "|1000000014|female|60s|  Korea|   Seoul|   Jongno-gu|contact with patient| 1000000013|            27|        2020-02-06|    2020-02-16|   2020-03-12|         null|released|\n",
      "|1000000015|  male|70s|  Korea|   Seoul|Seongdong-gu|    Seongdong-gu APT|       null|             8|        2020-02-11|    2020-02-19|         null|         null|released|\n",
      "|1000000016|  male|70s|  Korea|   Seoul|   Jongno-gu|contact with patient| 1000000017|          null|              null|    2020-02-19|   2020-03-11|         null|released|\n",
      "|1000000017|  male|70s|  Korea|   Seoul|   Jongno-gu|contact with patient| 1000000003|          null|              null|    2020-02-20|   2020-03-01|         null|released|\n",
      "|1000000018|  male|20s|  Korea|   Seoul|         etc|                 etc|       null|          null|              null|    2020-02-20|         null|         null|released|\n",
      "|1000000019|female|70s|  Korea|   Seoul|   Jongno-gu|contact with patient| 1000000021|          null|              null|    2020-02-20|   2020-03-08|         null|released|\n",
      "|1000000020|female|70s|  Korea|   Seoul|Seongdong-gu|    Seongdong-gu APT| 1000000015|          null|              null|    2020-02-20|         null|         null|released|\n",
      "+----------+------+---+-------+--------+------------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * from patients_tabel \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0da4318",
   "metadata": {},
   "source": [
    "### *Using SQL commands*, limit the output to only 5 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97911bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+---+-------+--------+-----------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+\n",
      "|patient_id|   sex|age|country|province|       city|      infection_case|infected_by|contact_number|symptom_onset_date|confirmed_date|released_date|deceased_date|   state|\n",
      "+----------+------+---+-------+--------+-----------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+\n",
      "|1000000001|  male|50s|  Korea|   Seoul| Gangseo-gu|     overseas inflow|       null|            75|        2020-01-22|    2020-01-23|   2020-02-05|         null|released|\n",
      "|1000000002|  male|30s|  Korea|   Seoul|Jungnang-gu|     overseas inflow|       null|            31|              null|    2020-01-30|   2020-03-02|         null|released|\n",
      "|1000000003|  male|50s|  Korea|   Seoul|  Jongno-gu|contact with patient| 2002000001|            17|              null|    2020-01-30|   2020-02-19|         null|released|\n",
      "|1000000004|  male|20s|  Korea|   Seoul|    Mapo-gu|     overseas inflow|       null|             9|        2020-01-26|    2020-01-30|   2020-02-15|         null|released|\n",
      "|1000000005|female|20s|  Korea|   Seoul|Seongbuk-gu|contact with patient| 1000000002|             2|              null|    2020-01-31|   2020-02-24|         null|released|\n",
      "+----------+------+---+-------+--------+-----------+--------------------+-----------+--------------+------------------+--------------+-------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT * from patients_tabel \"\"\").limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70edf17",
   "metadata": {},
   "source": [
    "### Select the count of males and females in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00e9fd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|   sex|count(sex)|\n",
      "+------+----------+\n",
      "|  null|         0|\n",
      "|  male|      1825|\n",
      "|female|      2218|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT sex, count(sex)  FROM patients_tabel GROUP BY sex ORDER BY count(sex)\n",
    "            \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73683674",
   "metadata": {},
   "source": [
    "### How many people did survive, and how many didn't?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e5a3908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+\n",
      "|   state|count(state)|\n",
      "+--------+------------+\n",
      "|deceased|          78|\n",
      "|isolated|        2158|\n",
      "|released|        2929|\n",
      "+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT state, count(state)  FROM patients_tabel GROUP BY state ORDER BY count(state)\n",
    "            \n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4424228",
   "metadata": {},
   "source": [
    "### Now, let's perform some preprocessing using SQL:\n",
    "1. Convert *age* column to double after removing the 's' at the end -- *hint: check SUBSTRING method*\n",
    "2. Select only the following columns: `['sex', 'age', 'province', 'state']`\n",
    "3. Store the result of the query in a new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3cf4f0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e2c76d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import translate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7560cf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a221f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|substring(age, 0, 2)|\n",
      "+--------------------+\n",
      "|                  50|\n",
      "|                  30|\n",
      "|                  50|\n",
      "|                  20|\n",
      "|                  20|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT substring(age, 0,2)  FROM patients_tabel \n",
    "            \n",
    "          \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff5095bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|translate(age, s, )|\n",
      "+-------------------+\n",
      "|                 50|\n",
      "|                 30|\n",
      "|                 50|\n",
      "|                 20|\n",
      "|                 20|\n",
      "+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate('age', 's', '')\n",
    "spark.sql(\"\"\"SELECT translate(age, 's', '')  FROM patients_tabel \n",
    "            \n",
    "          \"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6329dfbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o22.sql.\n: java.lang.UnsupportedOperationException: UPDATE TABLE is not supported temporarily.\r\n\tat org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:716)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\r\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\r\n\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)\r\n\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\r\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:391)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:104)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:104)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:97)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:117)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:117)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$simpleString$2(QueryExecution.scala:161)\r\n\tat org.apache.spark.sql.execution.ExplainUtils$.processPlan(ExplainUtils.scala:115)\r\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:161)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:206)\r\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:175)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13124/2154312928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m spark.sql(\"\"\"UPDATE patients_tabel SET age=translate(age, 's', '')\n\u001b[0m\u001b[0;32m      2\u001b[0m           \"\"\")\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36msql\u001b[1;34m(self, sqlQuery)\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'row3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m         \"\"\"\n\u001b[1;32m--> 723\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsparkSession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msqlQuery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wrapped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    724\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1309\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1310\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.sql.\n: java.lang.UnsupportedOperationException: UPDATE TABLE is not supported temporarily.\r\n\tat org.apache.spark.sql.execution.SparkStrategies$BasicOperators$.apply(SparkStrategies.scala:716)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$1(QueryPlanner.scala:63)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:489)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\r\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$3(QueryPlanner.scala:78)\r\n\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1(TraversableOnce.scala:162)\r\n\tat scala.collection.TraversableOnce.$anonfun$foldLeft$1$adapted(TraversableOnce.scala:162)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\r\n\tat scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:162)\r\n\tat scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:160)\r\n\tat scala.collection.AbstractIterator.foldLeft(Iterator.scala:1429)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.$anonfun$plan$2(QueryPlanner.scala:75)\r\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\r\n\tat org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:93)\r\n\tat org.apache.spark.sql.execution.SparkStrategies.plan(SparkStrategies.scala:67)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.createSparkPlan(QueryExecution.scala:391)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$sparkPlan$1(QueryExecution.scala:104)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:104)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:97)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executedPlan$1(QueryExecution.scala:117)\r\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:143)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:143)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:117)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:110)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$simpleString$2(QueryExecution.scala:161)\r\n\tat org.apache.spark.sql.execution.ExplainUtils$.processPlan(ExplainUtils.scala:115)\r\n\tat org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:161)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:206)\r\n\tat org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:175)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\r\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:228)\r\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:99)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)\r\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:618)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\r\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:613)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"UPDATE patients_tabel SET age=translate(age, 's', '')\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e918128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ALTER TABLE table_identifier ADD COLUMNS ( col_spec [ , ... ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61c70bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+--------+--------+\n",
      "|   sex|age|province|   state|\n",
      "+------+---+--------+--------+\n",
      "|  male|50s|   Seoul|released|\n",
      "|  male|30s|   Seoul|released|\n",
      "|  male|50s|   Seoul|released|\n",
      "|  male|20s|   Seoul|released|\n",
      "|female|20s|   Seoul|released|\n",
      "+------+---+--------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3=spark.sql(\"\"\"SELECT\n",
    "  sex\n",
    " ,age\n",
    " ,province\n",
    " ,state\n",
    "FROM patients_tabel\n",
    "\"\"\")\n",
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a24c24",
   "metadata": {},
   "source": [
    "## Machine Learning \n",
    "### Create a pipeline model to predict is_dead and evaluate the performance.\n",
    "- Use <b>StringIndexer</b> to transform <b>string</b> data type to indices.\n",
    "- Use <b>OneHotEncoder</b> to deal with categorical values.\n",
    "- Use <b>Imputer</b> to fill missing data with mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71806f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
